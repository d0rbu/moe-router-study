# SAE Evaluation Pipeline

This script provides a comprehensive evaluation pipeline for all SAE (Sparse Autoencoder) experiments generated by `exp/sae.py`.

## Overview

The `eval_all_saes.py` script:

1. **Discovers** all SAE experiment directories in the output folder
2. **Evaluates** each SAE using:
   - SAEBench evaluations (`sae_eval_saebench.py`)
   - Intruder detection evaluations (`eval_intruder.py`)
3. **Aggregates** results from all evaluations
4. **Generates rankings** based on each metric and overall performance
5. **Creates visualizations** including:
   - Overall ranking bar charts
   - Metric comparison heatmaps
   - Individual metric rankings
   - Hyperparameter correlation plots

## Usage

### Basic Usage

```bash
# Evaluate existing results only (no new evaluations)
uv run python -m exp.eval_all_saes --skip-evaluation

# Run both SAEBench and intruder evaluations, then aggregate
uv run python -m exp.eval_all_saes --run-saebench --run-intruder

# Run only SAEBench evaluation
uv run python -m exp.eval_all_saes --run-saebench

# Run only intruder evaluation
uv run python -m exp.eval_all_saes --run-intruder
```

### Advanced Options

```bash
uv run python -m exp.eval_all_saes \
  --model-name olmoe-i \
  --run-saebench \
  --run-intruder \
  --saebench-eval-types absorption autointerp core \
  --saebench-batchsize 512 \
  --intruder-n-tokens 10000000 \
  --intruder-batchsize 8 \
  --intruder-n-latents 1000 \
  --dtype bf16 \
  --seed 0
```

### Command Line Arguments

| Argument | Type | Default | Description |
|----------|------|---------|-------------|
| `--model-name` | str | `olmoe-i` | Model name for evaluation |
| `--run-saebench` | flag | False | Run SAEBench evaluation |
| `--run-intruder` | flag | False | Run intruder evaluation |
| `--saebench-eval-types` | list[str] | None | SAEBench evaluation types (absorption, autointerp, core, scr, tpp, sparse_probing, unlearning) |
| `--saebench-batchsize` | int | 512 | Batch size for SAEBench evaluation |
| `--intruder-n-tokens` | int | 10000000 | Number of tokens for intruder evaluation |
| `--intruder-batchsize` | int | 8 | Batch size for intruder evaluation |
| `--intruder-n-latents` | int | 1000 | Number of latents for intruder evaluation |
| `--dtype` | str | `bf16` | Data type for evaluation |
| `--seed` | int | 0 | Random seed |
| `--skip-evaluation` | flag | False | Skip running evaluations and only aggregate existing results |
| `--log-level` | str | `INFO` | Logging level (DEBUG, INFO, WARNING, ERROR) |

## Expected Input Structure

The script expects SAE experiments to be organized as follows:

```
out/
├── experiment_1/
│   ├── 0/
│   │   ├── config.json
│   │   └── ae.pt (or ae_*.pt)
│   ├── 1/
│   │   ├── config.json
│   │   └── ae.pt
│   └── ...
├── experiment_2/
│   ├── 0/
│   │   ├── config.json
│   │   └── ae.pt
│   └── ...
└── ...
```

Each subdirectory (numbered 0, 1, 2, ...) represents a different SAE configuration within the experiment.

## Output Structure

The script generates the following outputs in the `out/` directory:

### 1. Evaluation Results (`out/evaluation_results/`)

- `all_results.csv` - Complete results table in CSV format
- `all_results.json` - Complete results in JSON format
- `ranking_<metric>.csv` - Individual ranking tables for each metric
- `ranking_overall.csv` - Overall ranking based on all metrics

### 2. Visualizations (`out/visualizations/`)

- `overall_ranking.png` - Bar chart of top 20 SAEs by overall rank
- `metric_heatmap.png` - Heatmap comparing all metrics across SAEs
- `<metric>_ranking.png` - Top 15 SAEs for each individual metric
- `<hparam>_vs_<metric>.png` - Scatter plots showing hyperparameter correlations

## Metrics

### SAEBench Metrics

The script evaluates SAEs using various SAEBench benchmarks:
- **Absorption**: Measures feature absorption capabilities
- **Autointerp**: Automated interpretability scores
- **Core**: Core performance metrics
- **SCR**: Sparse Code Recovery
- **TPP**: Token Prediction Performance
- **Sparse Probing**: Sparse probing accuracy
- **Unlearning**: Unlearning capabilities

### Intruder Metrics

The intruder evaluation measures how well the SAE latents can distinguish between relevant and irrelevant examples:
- **Average Score**: Mean intruder detection score across all latents
- **Standard Deviation**: Consistency of intruder detection

## Rankings

The script generates several types of rankings:

1. **Individual Metric Rankings**: Rankings based on each evaluation metric
2. **Overall Ranking**: Combined ranking based on average rank across all metrics (lower is better)

Rankings are saved as CSV files and visualized in various formats.

## Example Workflow

1. Train multiple SAEs with different hyperparameters using `exp/sae.py`:
   ```bash
   uv run python -m exp.sae --expansion-factor 8 16 32 --k 64 128 --layer 5 7 9
   ```

2. Run the evaluation pipeline:
   ```bash
   uv run python -m exp.eval_all_saes --run-saebench --run-intruder
   ```

3. View the results:
   - Check `out/evaluation_results/all_results.csv` for the full table
   - Check `out/evaluation_results/ranking_overall.csv` for the best SAEs
   - Browse `out/visualizations/` for visual comparisons

4. Re-aggregate results without re-running evaluations:
   ```bash
   uv run python -m exp.eval_all_saes --skip-evaluation
   ```

## Notes

- The script automatically handles missing evaluation results - if an evaluation hasn't been run for an experiment, those metrics will be marked as missing
- Large evaluations (especially intruder with many tokens/latents) can take significant time and GPU resources
- You can run evaluations incrementally by using `--run-saebench` or `--run-intruder` separately
- The `--skip-evaluation` flag is useful for quickly regenerating visualizations or re-computing rankings after manual edits to result files

## Troubleshooting

### No experiments found
- Verify that your output directory (`out/`) contains experiment subdirectories
- Check that each SAE configuration has both `config.json` and `ae.pt` files

### Evaluation failures
- Check the log output for specific error messages
- Ensure you have sufficient GPU memory for the evaluation batch sizes
- Try reducing `--saebench-batchsize` or `--intruder-batchsize` if you encounter OOM errors

### Missing metrics in results
- Some metrics may only be available after running specific evaluations
- Use `--run-saebench` and `--run-intruder` to populate all metrics
- Check that evaluations completed successfully (no errors in logs)

