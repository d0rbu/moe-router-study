"""
Evaluate CAA steering vectors using router path interventions.

This script evaluates steering vectors generated by capital_country_generate_path_caa.py
using the same evaluation methodology as the CAA paper, but applying interventions
to router logits instead of residual stream activations.

Usage:
    # Evaluate on A/B questions
    uv run python -m exp.eval_caa_router_steering \\
        --intervention-path "out/intervention_paths/sycophancy_caa.pt" \\
        --model-name "olmoe-i" \\
        --eval-type "ab" \\
        --multipliers -1 0 1 \\
        --behavior "sycophancy"

    # Evaluate on open-ended questions
    uv run python -m exp.eval_caa_router_steering \\
        --intervention-path "out/intervention_paths/sycophancy_caa.pt" \\
        --model-name "olmoe-i" \\
        --eval-type "open_ended" \\
        --multipliers -2 -1 0 1 2 \\
        --behavior "sycophancy"
"""

import json
from pathlib import Path
import sys

import arguably
from loguru import logger
from nnterp import StandardizedTransformer
import torch as th
import torch.nn.functional as F
from tqdm import tqdm

from CAA.behaviors import (
    get_ab_test_data,
    get_mmlu_data,
    get_open_ended_test_data,
    get_results_dir,
    get_system_prompt,
    get_truthful_qa_data,
)
from CAA.utils.helpers import get_a_b_probs
from core.dtype import get_dtype
from core.model import get_model_config


def load_intervention_path(path: str, device: th.device) -> th.Tensor:
    """
    Load intervention path from a .pt file.

    Args:
        path: Path to the .pt file
        device: Device to load the tensor to

    Returns:
        Intervention path tensor of shape (L, E)
    """
    logger.debug(f"Loading intervention path from {path}")
    file_path = Path(path)
    if not file_path.exists():
        raise ValueError(f"Intervention path file not found: {path}")

    logger.trace(f"Loading tensor from {file_path} to device {device}")
    data = th.load(file_path, map_location=device, weights_only=False)

    if isinstance(data, dict):
        logger.debug(f"Loaded dict with keys: {list(data.keys())}")
        if "intervention_path" in data:
            tensor = data["intervention_path"]
            logger.trace(
                f"Extracted intervention_path tensor with shape {tensor.shape}"
            )
        else:
            raise ValueError(
                f"File {path} must contain 'intervention_path' key. "
                f"Found keys: {list(data.keys())}"
            )
    elif isinstance(data, th.Tensor):
        logger.debug(f"Loaded tensor directly with shape {data.shape}")
        tensor = data
    else:
        raise ValueError(f"File {path} must contain a dict or tensor, got {type(data)}")

    result = tensor.to(device=device, dtype=th.float32)
    logger.debug(
        f"Intervention tensor loaded: shape={result.shape}, dtype={result.dtype}, device={result.device}"
    )
    return result


def find_instruction_end_position(tokens: th.Tensor, end_str_tokens: th.Tensor) -> int:
    """
    Find the position where the instruction ends (for CAA-style intervention).

    Args:
        tokens: Token sequence
        end_str_tokens: Tokens that mark the end of instruction (e.g., [/INST])

    Returns:
        Position index where instruction ends, or -1 if not found
    """
    seq_len = tokens.shape[0]
    end_len = end_str_tokens.shape[0]

    logger.trace(f"Finding instruction end: seq_len={seq_len}, end_len={end_len}")

    if end_len > seq_len:
        logger.debug(
            f"End marker length {end_len} > sequence length {seq_len}, returning -1"
        )
        return -1

    for i in range(seq_len - end_len, -1, -1):
        if th.equal(tokens[i : i + end_len], end_str_tokens):
            pos = i + end_len - 1
            logger.debug(f"Found instruction end marker at position {pos}")
            return pos

    logger.debug("Instruction end marker not found, returning -1")
    return -1


def apply_router_intervention(
    model: StandardizedTransformer,
    batch: dict,
    intervention_tensor: th.Tensor,  # (L, E)
    multiplier: float,
    layers_with_routers: list[int],
    apply_from_instruction_end: bool = True,
) -> None:
    """
    Apply router intervention by modifying router logits.

    For each router layer, subtract multiplier * intervention_tensor[layer] from router logits.
    Following CAA, applies intervention to all positions after the instruction end.

    Args:
        model: The MoE model
        batch: Input batch dictionary
        intervention_tensor: Intervention tensor of shape (L, E)
        multiplier: Scaling factor for intervention
        layers_with_routers: List of layer indices with routers
        apply_from_instruction_end: If True, only apply after instruction end (CAA style)
    """
    logger.trace(
        f"Applying router intervention: multiplier={multiplier}, apply_from_instruction_end={apply_from_instruction_end}"
    )

    if multiplier == 0.0:
        logger.debug("Multiplier is 0.0, skipping intervention")
        return  # No intervention

    # Find instruction end position if needed
    instruction_end_pos = -1
    if apply_from_instruction_end:
        # Try to find [/INST] or similar instruction end marker
        end_markers = ["[/INST]", "<|assistant|>", "\n\n"]
        input_ids = batch["input_ids"][0]  # (T,)
        logger.trace(
            f"Searching for instruction end marker in sequence of length {input_ids.shape[0]}"
        )

        for marker in end_markers:
            marker_tokens = model.tokenizer.encode(marker, add_special_tokens=False)
            if marker_tokens:
                marker_tensor = th.tensor(marker_tokens, device=input_ids.device)
                instruction_end_pos = find_instruction_end_position(
                    input_ids, marker_tensor
                )
                if instruction_end_pos >= 0:
                    logger.debug(
                        f"Found instruction end marker '{marker}' at position {instruction_end_pos}"
                    )
                    break

        if instruction_end_pos < 0:
            logger.warning(
                "Could not find instruction end marker, applying intervention to all positions"
            )

    batch_size, seq_len = batch["input_ids"].shape
    logger.trace(f"Processing batch: batch_size={batch_size}, seq_len={seq_len}")

    for layer_position, layer_idx in enumerate(layers_with_routers):
        logger.trace(f"Processing layer {layer_idx} (position {layer_position})")
        router_output = model.routers_output[layer_idx]

        # Handle different router output formats
        if isinstance(router_output, tuple):
            if len(router_output) == 2:
                router_scores, _router_indices = router_output
                logger.trace(f"Layer {layer_idx}: router output is 2-tuple")
            elif len(router_output) == 3:
                (
                    original_router_logits,
                    original_router_weights,
                    original_router_indices,
                ) = router_output
                router_scores = original_router_logits
                logger.trace(f"Layer {layer_idx}: router output is 3-tuple")
            else:
                raise ValueError(
                    f"Found tuple of length {len(router_output)} for router output at layer {layer_idx}"
                )
        else:
            router_scores = router_output
            logger.trace(f"Layer {layer_idx}: router output is not a tuple")

        # Get router logits
        router_logits = router_scores.save()
        logger.trace(
            f"Layer {layer_idx}: router_logits shape before reshape: {router_logits.shape}"
        )

        # Reshape: router_logits from .save() might be (B*T, E) or (B, T, E)
        if router_logits.dim() == 2:
            # Flattened format: (B*T, E)
            router_logits = router_logits.reshape(batch_size, seq_len, -1)
        elif router_logits.dim() == 3:
            # Already in (B, T, E) format
            pass
        else:
            raise ValueError(
                f"Unexpected router_logits shape {router_logits.shape} at layer {layer_idx}"
            )

        logger.trace(
            f"Layer {layer_idx}: router_logits shape after reshape: {router_logits.shape}"
        )

        # Validate layer_position is within bounds
        if layer_position >= intervention_tensor.shape[0]:
            raise ValueError(
                f"Layer position {layer_position} >= intervention_tensor layers {intervention_tensor.shape[0]}"
            )

        # Get intervention for this layer
        layer_intervention = intervention_tensor[layer_position].to(
            device=router_logits.device, dtype=router_logits.dtype
        )  # (E,)
        logger.trace(
            f"Layer {layer_idx}: intervention vector shape={layer_intervention.shape}, "
            f"min={layer_intervention.min().item():.4f}, max={layer_intervention.max().item():.4f}, "
            f"mean={layer_intervention.mean().item():.4f}"
        )

        # Apply intervention: subtract from positions after instruction end
        # This matches CAA which adds to all positions after instruction
        modified_logits = router_logits.clone()
        intervention_vector = multiplier * layer_intervention.unsqueeze(0).unsqueeze(
            0
        )  # (1, 1, E)

        if apply_from_instruction_end and instruction_end_pos >= 0:
            # Create mask for positions after instruction end
            position_mask = (
                th.arange(seq_len, device=router_logits.device) > instruction_end_pos
            )
            num_intervened_positions = position_mask.sum().item()
            logger.debug(
                f"Layer {layer_idx}: Applying intervention to {num_intervened_positions}/{seq_len} positions "
                f"(after position {instruction_end_pos})"
            )
            position_mask = position_mask.unsqueeze(0).unsqueeze(-1)  # (1, T, 1)
            modified_logits -= intervention_vector * position_mask.float()
        else:
            # Apply to all positions
            logger.debug(
                f"Layer {layer_idx}: Applying intervention to all {seq_len} positions"
            )
            modified_logits -= intervention_vector  # Broadcast to (B, T, E)

        logger.trace(
            f"Layer {layer_idx}: modified_logits min={modified_logits.min().item():.4f}, "
            f"max={modified_logits.max().item():.4f}, mean={modified_logits.mean().item():.4f}"
        )

        # Update router output
        if isinstance(router_output, tuple):
            if len(router_output) == 2:
                model.routers_output[layer_idx] = (
                    modified_logits.reshape(-1, modified_logits.shape[-1]),
                    router_output[1],
                )
            elif len(router_output) == 3:
                # Recompute top-k for all positions
                top_k = model.config.num_experts_per_tok
                logger.trace(f"Layer {layer_idx}: Recomputing top-{top_k} experts")
                all_new_weights, all_new_indices = th.topk(
                    modified_logits, k=top_k, dim=-1
                )  # (B, T, top_k)
                all_new_weights = F.softmax(all_new_weights, dim=-1)

                model.routers_output[layer_idx] = (
                    modified_logits.reshape(-1, modified_logits.shape[-1]),
                    all_new_weights.reshape(-1, all_new_weights.shape[-1]),
                    all_new_indices.reshape(-1, all_new_indices.shape[-1]),
                )
        else:
            model.routers_output[layer_idx] = modified_logits.reshape(
                -1, modified_logits.shape[-1]
            )

    logger.debug(
        f"Router intervention applied successfully to {len(layers_with_routers)} layers"
    )


def process_item_ab(
    item: dict[str, str],
    model: StandardizedTransformer,
    intervention_tensor: th.Tensor,
    multiplier: float,
    layers_with_routers: list[int],
    system_prompt: str | None,
    a_token_id: int,
    b_token_id: int,
) -> dict[str, str]:
    """
    Process an A/B question item.

    Args:
        item: Dictionary with 'question', 'answer_matching_behavior', 'answer_not_matching_behavior'
        model: The MoE model
        intervention_tensor: Intervention tensor (L, E)
        multiplier: Intervention multiplier
        layers_with_routers: List of router layer indices
        system_prompt: Optional system prompt
        a_token_id: Token ID for "A"
        b_token_id: Token ID for "B"

    Returns:
        Dictionary with question, answers, and probabilities
    """
    question = item["question"]
    answer_matching_behavior = item["answer_matching_behavior"]
    answer_not_matching_behavior = item["answer_not_matching_behavior"]

    logger.trace(
        f"Processing A/B item: question length={len(question)}, "
        f"matching={answer_matching_behavior}, not_matching={answer_not_matching_behavior}"
    )

    # Format prompt: question + "(" to get logits at the answer position
    if system_prompt:
        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": question},
            {"role": "assistant", "content": "("},
        ]
        logger.debug("Using system prompt for A/B question")
    else:
        messages = [
            {"role": "user", "content": question},
            {"role": "assistant", "content": "("},
        ]

    formatted = model.tokenizer.apply_chat_template(
        messages, tokenize=False, add_generation_prompt=False
    )
    tokens = model.tokenizer(formatted, return_tensors="pt").input_ids.to(model.device)
    logger.trace(f"Tokenized prompt: {tokens.shape[1]} tokens")

    batch = {"input_ids": tokens}

    # Apply intervention
    with model.trace(batch):
        apply_router_intervention(
            model,
            batch,
            intervention_tensor,
            multiplier,
            layers_with_routers,
            apply_from_instruction_end=True,
        )
        logits = model.lm_head.output.save()

    # Get probabilities for A and B
    a_prob, b_prob = get_a_b_probs(logits, a_token_id, b_token_id)
    logger.debug(f"A/B probabilities: A={a_prob:.4f}, B={b_prob:.4f}")

    return {
        "question": question,
        "answer_matching_behavior": answer_matching_behavior,
        "answer_not_matching_behavior": answer_not_matching_behavior,
        "a_prob": a_prob,
        "b_prob": b_prob,
    }


def process_item_open_ended(
    item: dict[str, str],
    model: StandardizedTransformer,
    intervention_tensor: th.Tensor,
    multiplier: float,
    layers_with_routers: list[int],
    system_prompt: str | None,
    max_new_tokens: int = 100,
) -> dict[str, str]:
    """
    Process an open-ended question item.

    Uses a simplified generation approach that applies intervention at each step.
    For full generation with KV caching, see capital_country_chat.py.

    Args:
        item: Dictionary with 'question'
        model: The MoE model
        intervention_tensor: Intervention tensor (L, E)
        multiplier: Intervention multiplier
        layers_with_routers: List of router layer indices
        system_prompt: Optional system prompt
        max_new_tokens: Maximum tokens to generate

    Returns:
        Dictionary with question and model output
    """
    question = item["question"]

    # Format prompt
    if system_prompt:
        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": question},
        ]
    else:
        messages = [{"role": "user", "content": question}]

    formatted = model.tokenizer.apply_chat_template(
        messages, tokenize=False, add_generation_prompt=True
    )
    input_ids = model.tokenizer(formatted, return_tensors="pt").input_ids.to(
        model.device
    )

    # Simple generation loop (without KV caching for simplicity)
    # For production, use the full generation function from capital_country_chat.py
    logger.debug(f"Starting open-ended generation: max_new_tokens={max_new_tokens}")
    generated_tokens = []
    current_ids = input_ids
    eos_token_id = model.tokenizer.eos_token_id
    initial_seq_len = input_ids.shape[1]
    logger.trace(f"Initial sequence length: {initial_seq_len}")

    for step in range(max_new_tokens):
        logger.trace(
            f"Generation step {step + 1}/{max_new_tokens}, current length: {current_ids.shape[1]}"
        )
        batch = {"input_ids": current_ids}

        with model.trace(batch):
            apply_router_intervention(
                model,
                batch,
                intervention_tensor,
                multiplier,
                layers_with_routers,
                apply_from_instruction_end=True,
            )
            logits = model.lm_head.output.save()

        # Get next token (greedy decoding)
        next_token_logits = logits[0, -1, :]
        next_token_id = th.argmax(next_token_logits).item()
        next_token_prob = F.softmax(next_token_logits, dim=-1)[next_token_id].item()
        logger.trace(
            f"Step {step + 1}: sampled token_id={next_token_id}, prob={next_token_prob:.4f}"
        )

        if next_token_id == eos_token_id:
            logger.debug(f"EOS token generated at step {step + 1}, stopping generation")
            break

        generated_tokens.append(next_token_id)
        current_ids = th.cat(
            [
                current_ids,
                th.tensor(
                    [[next_token_id]], device=model.device, dtype=current_ids.dtype
                ),
            ],
            dim=1,
        )

        # Safety check: prevent extremely long sequences
        if current_ids.shape[1] > initial_seq_len + max_new_tokens * 2:
            logger.warning("Sequence length exceeded safety limit, stopping generation")
            break

    # Decode generated tokens
    model_output = model.tokenizer.decode(generated_tokens, skip_special_tokens=True)
    logger.debug(
        f"Generated {len(generated_tokens)} tokens, output length: {len(model_output)}"
    )

    return {
        "question": question,
        "model_output": model_output,
        "raw_model_output": formatted + model_output,
    }


def process_item_tqa_mmlu(
    item: dict[str, str],
    model: StandardizedTransformer,
    intervention_tensor: th.Tensor,
    multiplier: float,
    layers_with_routers: list[int],
    system_prompt: str | None,
    a_token_id: int,
    b_token_id: int,
) -> dict[str, str]:
    """
    Process a TruthfulQA or MMLU item.

    Args:
        item: Dictionary with 'prompt', 'correct', 'incorrect', 'category'
        model: The MoE model
        intervention_tensor: Intervention tensor (L, E)
        multiplier: Intervention multiplier
        layers_with_routers: List of router layer indices
        system_prompt: Optional system prompt
        a_token_id: Token ID for "A"
        b_token_id: Token ID for "B"

    Returns:
        Dictionary with question, correct/incorrect answers, probabilities, and category
    """
    prompt = item["prompt"]
    correct = item["correct"]
    incorrect = item["incorrect"]
    category = item.get("category", "")

    # Format prompt: prompt + "(" to get logits at the answer position
    if system_prompt:
        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": prompt},
            {"role": "assistant", "content": "("},
        ]
    else:
        messages = [
            {"role": "user", "content": prompt},
            {"role": "assistant", "content": "("},
        ]

    formatted = model.tokenizer.apply_chat_template(
        messages, tokenize=False, add_generation_prompt=False
    )
    tokens = model.tokenizer(formatted, return_tensors="pt").input_ids.to(model.device)

    batch = {"input_ids": tokens}

    # Apply intervention
    with model.trace(batch):
        apply_router_intervention(
            model,
            batch,
            intervention_tensor,
            multiplier,
            layers_with_routers,
            apply_from_instruction_end=True,
        )
        logits = model.lm_head.output.save()

    # Get probabilities for A and B
    a_prob, b_prob = get_a_b_probs(logits, a_token_id, b_token_id)

    return {
        "question": prompt,
        "correct": correct,
        "incorrect": incorrect,
        "a_prob": a_prob,
        "b_prob": b_prob,
        "category": category,
    }


@arguably.command()
def eval_caa_router_steering(
    *,
    intervention_path: str,
    model_name: str = "olmoe-i",
    model_step_ckpt: int | None = None,
    model_dtype: str = "bf16",
    eval_type: str = "ab",
    behavior: str = "",
    multipliers: list[float] | None = None,
    system_prompt: str | None = None,
    output_dir: str = "",
    hf_token: str = "",
    max_new_tokens: int = 100,
    overwrite: bool = False,
    log_level: str = "INFO",
) -> None:
    """
    Evaluate CAA steering vectors using router path interventions.

    Args:
        intervention_path: Path to the .pt file with steering vector
        model_name: Name of the model to use
        model_step_ckpt: Checkpoint step to load (None for latest)
        model_dtype: Data type for model weights
        eval_type: Evaluation type (ab, open_ended, truthful_qa, mmlu)
        behavior: Behavior name (for dataset loading and results directory)
        multipliers: List of multipliers to test
        system_prompt: System prompt direction ("pos", "neg", or None)
        output_dir: Output directory for results (default: CAA/results/<behavior>)
        hf_token: Hugging Face API token
        max_new_tokens: Maximum tokens for open-ended generation
        overwrite: Whether to overwrite existing results
        log_level: Logging level
    """
    # Setup logging
    logger.remove()
    logger.add(sys.stderr, level=log_level)
    logger.info(f"Running eval_caa_router_steering with log level: {log_level}")

    if not multipliers:
        multipliers = [-1.0, 0.0, 1.0]

    # Validate eval_type
    valid_eval_types = ["ab", "open_ended", "truthful_qa", "mmlu"]
    if eval_type not in valid_eval_types:
        raise ValueError(
            f"Invalid eval_type: {eval_type}. Must be one of {valid_eval_types}"
        )

    # Get model configuration
    model_config = get_model_config(model_name)
    model_ckpt = model_config.get_checkpoint_strict(step=model_step_ckpt)
    model_dtype_torch = get_dtype(model_dtype)

    logger.info(f"Loading model: {model_config.hf_name}")
    logger.info(f"Checkpoint: {model_ckpt}")

    # Load model
    print(f"⏳ Loading model {model_config.hf_name}...")
    model = StandardizedTransformer(
        model_config.hf_name,
        check_attn_probs_with_trace=False,
        check_renaming=False,
        revision=str(model_ckpt),
        device_map={"": "cuda"},
        torch_dtype=model_dtype_torch,
        token=hf_token,
    )
    print("✅ Model loaded!")

    # Get model architecture info
    layers_with_routers = list(model.layers_with_routers)
    num_layers = len(layers_with_routers)
    num_experts = model.config.num_experts

    logger.info(f"Number of layers with routers: {num_layers}")
    logger.info(f"Number of experts per layer: {num_experts}")

    # Load intervention path
    logger.info("=" * 80)
    logger.info("Loading intervention path")
    logger.info("=" * 80)

    intervention_tensor = load_intervention_path(intervention_path, model.device)
    logger.info(f"Intervention tensor shape: {intervention_tensor.shape}")

    # Validate shape
    if intervention_tensor.shape != (num_layers, num_experts):
        raise ValueError(
            f"Intervention tensor shape {intervention_tensor.shape} does not match "
            f"model architecture ({num_layers}, {num_experts})"
        )

    # Get token IDs for A and B
    a_token_id = model.tokenizer.convert_tokens_to_ids("A")
    b_token_id = model.tokenizer.convert_tokens_to_ids("B")

    logger.debug(f"Token IDs: A={a_token_id}, B={b_token_id}")

    if a_token_id is None or b_token_id is None:
        raise ValueError("Could not find token IDs for 'A' and 'B'")

    # Load test dataset
    logger.info("=" * 80)
    logger.info("Loading test dataset")
    logger.info("=" * 80)

    if eval_type in ["ab", "open_ended"]:
        if not behavior:
            raise ValueError(f"behavior is required for eval_type '{eval_type}'")
        if eval_type == "ab":
            test_data = get_ab_test_data(behavior)
        else:
            test_data = get_open_ended_test_data(behavior)
    elif eval_type == "truthful_qa":
        test_data = get_truthful_qa_data()
    elif eval_type == "mmlu":
        test_data = get_mmlu_data()
    else:
        raise ValueError(f"Unknown eval_type: {eval_type}")

    logger.info(f"Loaded {len(test_data)} test items")
    if test_data:
        logger.debug(f"Test data sample keys: {list(test_data[0].keys())}")

    # Get system prompt
    system_prompt_text = None
    if system_prompt and behavior:
        system_prompt_text = get_system_prompt(behavior, system_prompt)
        logger.debug(
            f"Using system prompt: {system_prompt_text[:100]}..."
            if system_prompt_text
            else "No system prompt"
        )
    else:
        logger.debug("No system prompt configured")

    # Get output directory
    if not output_dir:
        if behavior:
            output_dir = get_results_dir(behavior)
        else:
            output_dir = "out/caa_router_steering_results"
    output_path = Path(output_dir)
    output_path.mkdir(parents=True, exist_ok=True)
    logger.debug(f"Output directory: {output_path}")

    # Process each multiplier
    for multiplier in multipliers:
        logger.info("=" * 80)
        logger.info(f"Processing multiplier: {multiplier}")
        logger.info("=" * 80)

        # Create result filename
        result_filename = (
            f"results_router_steering_{eval_type}_multiplier={multiplier}.json"
        )
        if behavior:
            result_filename = f"results_router_steering_{behavior}_{eval_type}_multiplier={multiplier}.json"
        if system_prompt:
            result_filename = result_filename.replace(
                ".json", f"_system_prompt={system_prompt}.json"
            )

        result_path = output_path / result_filename

        if result_path.exists() and not overwrite:
            logger.info(f"Results already exist at {result_path}, skipping...")
            continue

        # Process items
        process_methods = {
            "ab": process_item_ab,
            "open_ended": process_item_open_ended,
            "truthful_qa": process_item_tqa_mmlu,
            "mmlu": process_item_tqa_mmlu,
        }

        process_fn = process_methods[eval_type]

        results = []
        logger.info(f"Processing {len(test_data)} items with multiplier {multiplier}")
        for item_idx, item in enumerate(
            tqdm(test_data, desc=f"Multiplier {multiplier}")
        ):
            logger.trace(f"Processing item {item_idx + 1}/{len(test_data)}")
            try:
                if eval_type == "ab":
                    result = process_fn(
                        item,
                        model,
                        intervention_tensor,
                        multiplier,
                        layers_with_routers,
                        system_prompt_text,
                        a_token_id,
                        b_token_id,
                    )
                elif eval_type == "open_ended":
                    result = process_fn(
                        item,
                        model,
                        intervention_tensor,
                        multiplier,
                        layers_with_routers,
                        system_prompt_text,
                        max_new_tokens,
                    )
                else:  # truthful_qa or mmlu
                    result = process_fn(
                        item,
                        model,
                        intervention_tensor,
                        multiplier,
                        layers_with_routers,
                        system_prompt_text,
                        a_token_id,
                        b_token_id,
                    )
                results.append(result)
            except Exception as e:
                logger.error(
                    f"Error processing item {item_idx + 1}: {e}", exc_info=True
                )
                # Continue with other items, but log the error
                results.append(
                    {
                        "error": str(e),
                        "item_index": item_idx,
                        "item": item.get("question", item.get("prompt", "unknown")),
                    }
                )

        # Save results
        with open(result_path, "w") as f:
            json.dump(results, f, indent=4)

        logger.info(f"✅ Saved results to {result_path}")

    logger.info("=" * 80)
    logger.info("EVALUATION COMPLETE")
    logger.info("=" * 80)


if __name__ == "__main__":
    arguably.run()
